{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def read_label_map(label_map_path):\n",
    "\n",
    "    item_id = None\n",
    "    item_name = None\n",
    "    items = {}\n",
    "    \n",
    "    class_labels = np.sort(os.listdir(label_map_path))[3:]\n",
    "    for i, label in enumerate(class_labels):\n",
    "        items[i] = label\n",
    "    items[i+1] = \"happens\"\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "items = read_label_map(\"/mnt/tmp/JHMDB/Frames\")\n",
    "\n",
    "# main_path = '/tmp2'\n",
    "images_path = '/mnt/tmp/JHMDB/Frames'\n",
    "vis_save_path = './output_images'\n",
    " \n",
    "# TubeR's csn152, ava21 results\n",
    "detection = '../tmp_jhmdb/0.txt' #numbers are changeable\n",
    "gt = '../tmp_jhmdb/GT_0.txt'\n",
    "\n",
    "query_num = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dict = {}\n",
    "with open(gt) as f:\n",
    "    for line in f.readlines():\n",
    "        img_id = line.split(' ')[0]\n",
    "        annotation = [int(float(n)) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "        one_hot_obj_label = annotation[6:]\n",
    "        obj_labels = [items[i] for i, e in enumerate(one_hot_obj_label) if e!=0]\n",
    "        coord = annotation[2:6]\n",
    "        if img_id not in anno_dict.keys():\n",
    "            anno_dict[img_id] = {\n",
    "                \"obj\": [obj_labels],\n",
    "                \"coord\": [coord]\n",
    "            }\n",
    "        else:\n",
    "            anno_dict[img_id][\"obj\"].append(obj_labels)\n",
    "            anno_dict[img_id][\"coord\"].append(coord)\n",
    "\n",
    "# print(\"test if anno_dict is well constructed: \", anno_dict['_dBTTYDRdRQ_1528'])\n",
    "\n",
    "pred_dict = {}\n",
    "with open(detection) as f:\n",
    "    for line in f.readlines(): #[0::15]:\n",
    "        img_id = line.split(' ')[0]\n",
    "        # print(line)\n",
    "        annotation = [float(n) for n in line.split('[')[1].split(']')[0].split(',')]\n",
    "        one_hot_obj_label = annotation[4:-1]\n",
    "        # print(len(one_hot_obj_label))\n",
    "        obj_labels = [items[i] for i, e in enumerate([k>0.7 for k in one_hot_obj_label]) if e]\n",
    "        # print([k>1e-30 for k in multi_hot_obj_label])\n",
    "        if len(obj_labels)==0:\n",
    "            continue\n",
    "        coord = annotation[:4]\n",
    "        if img_id not in pred_dict.keys():\n",
    "            pred_dict[img_id] = {\n",
    "                \"obj\": [obj_labels],\n",
    "                \"coord\": [coord]\n",
    "            }\n",
    "        else:\n",
    "            pred_dict[img_id][\"obj\"].append(obj_labels)\n",
    "            pred_dict[img_id][\"coord\"].append(coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "import PIL\n",
    "import os\n",
    "import torch\n",
    "# import sys\n",
    "# module_path = \"./visualization_utils.py\"\n",
    "# sys.path.append(module_path)\n",
    "import visualization_utils_custom as vis_utils\n",
    "import datasets.video_transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "import models.transformer.util.box_ops as box_ops\n",
    "\n",
    "%matplotlib inline\n",
    "number = 0\n",
    "well_localized = 0\n",
    "exclude_counts = 0\n",
    "misclassified = 0\n",
    "for image in list(anno_dict.keys())[::2]:\n",
    "    temp_cls = \"_\".join(image.split('_')[:2])\n",
    "    classes_ = [\"brush_hair\", \"kick_ball\", \"shoot_ball\", \"shoot_bow\", \"shoot_gun\", \"climb_stairs\", \"swing_baseball\"]\n",
    "    cls = temp_cls if temp_cls in classes_ else temp_cls.split('_')[0]\n",
    "    vid = \"_\".join(image.split('_')[2:]) if temp_cls in classes_ else \"_\".join(image.split('_')[1:])\n",
    "    vid = \"-\".join(vid.split('-')[:-1])\n",
    "    image_num = image.split('-')[-1]\n",
    "    img_path = os.path.join(images_path, cls, vid, str(int(image_num)).zfill(5)+\".png\")\n",
    "    img = PIL.Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    if w >= h:\n",
    "        nh = 224\n",
    "        nw = 224 * w / h\n",
    "    else:\n",
    "        nw = 224\n",
    "        nh = 224 * h / w\n",
    "\n",
    "    # crop_top = int(round((h - nh) / 2.))\n",
    "    # crop_left = int(round((w - nw) / 2.))\n",
    "\n",
    "    img = img.resize((int(nw), int(nh)))\n",
    "    # img = F.crop(img, crop_top, crop_left, nh, nw)\n",
    "    \n",
    "    try:\n",
    "        num_gt_instances = len(anno_dict[image][\"obj\"])\n",
    "        for i, (gt_obj, pred_obj, gt_coord, pred_coord) in enumerate(zip(anno_dict[image][\"obj\"], pred_dict[image][\"obj\"], anno_dict[image][\"coord\"], pred_dict[image][\"coord\"])):\n",
    "            gt_cat = str(gt_obj)\n",
    "            pred_cat = str(pred_obj)\n",
    "            gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_coord[0], gt_coord[1], gt_coord[2], gt_coord[3]\n",
    "            pred_xmin, pred_ymin, pred_xmax, pred_ymax = pred_coord[0], pred_coord[1], pred_coord[2], pred_coord[3]\n",
    "            vis_utils.draw_bounding_box_on_image(\n",
    "                img, gt_ymin, gt_xmin, gt_ymax, gt_xmax,\n",
    "                color = 'Green',\n",
    "                display_str_list=[gt_cat],\n",
    "                use_normalized_coordinates=False\n",
    "            )\n",
    "            vis_utils.draw_bounding_box_on_image(\n",
    "                img, pred_ymin, pred_xmin, pred_ymax, pred_xmax,\n",
    "                color = 'Yellow',\n",
    "                display_str_list=[pred_cat],\n",
    "                use_normalized_coordinates=False,\n",
    "                margin2=20\n",
    "            )            \n",
    "        # iou = box_ops.box_iou(torch.tensor([gt_xmin, gt_ymin, gt_xmax, gt_ymax]).unsqueeze(0), torch.tensor([pred_xmin, pred_ymin, pred_xmax, pred_ymax]).unsqueeze(0))[0]\n",
    "        # if iou >= 0.5:\n",
    "        #     # print(\"localized well\")\n",
    "        #     well_localized += 1\n",
    "        if pred_cat != gt_cat:\n",
    "            print(\"image id: \", image)\n",
    "            display(img)\n",
    "            misclassified += 1\n",
    "        # else:\n",
    "        #     print(\"image id: \", image)\n",
    "        #     display(img)\n",
    "        #     continue\n",
    "        # print(number)\n",
    "        # print(\"number of GT instances: \", num_gt_instances)\n",
    "        # print(\"number of predictions: \", num_predicted_instances)\n",
    "        number += 1\n",
    "        # del img\n",
    "    except:\n",
    "        # print(\"excluded key frame encountered, skip to the next frame\")\n",
    "        exclude_counts += 1\n",
    "        # user_input = str(input())\n",
    "        # if user_input == \"q\": break\n",
    "        # else: pass\n",
    "print(misclassified, len(anno_dict.keys())-exclude_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_ops.box_iou(torch.tensor([gt_xmin, gt_ymin, gt_xmax, gt_ymax]).unsqueeze(0), torch.tensor([pred_xmin, pred_ymin, pred_xmax, pred_ymax]).unsqueeze(0))[0] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "import PIL\n",
    "import os\n",
    "# import sys\n",
    "# module_path = \"./visualization_utils.py\"\n",
    "# sys.path.append(module_path)\n",
    "import visualization_utils_custom as vis_utils\n",
    "%matplotlib inline\n",
    "number = 0\n",
    "for image in list(anno_dict.keys())[:1500]:\n",
    "    temp_cls = \"_\".join(image.split('_')[:2])\n",
    "    classes_ = [\"brush_hair\", \"kick_ball\", \"shoot_ball\", \"shoot_bow\", \"shoot_gun\", \"climb_stairs\", \"swing_baseball\"]\n",
    "    cls = temp_cls if temp_cls in classes_ else temp_cls.split('_')[0]\n",
    "    vid = \"_\".join(image.split('_')[2:]) if temp_cls in classes_ else \"_\".join(image.split('_')[1:])\n",
    "    vid = \"-\".join(vid.split('-')[:-1])\n",
    "    image_num = image.split('-')[-1]\n",
    "    img_path = os.path.join(images_path, cls, vid, str(int(image_num)).zfill(5)+\".png\")\n",
    "    img = PIL.Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    if w >= h:\n",
    "        nh = 224\n",
    "        nw = 224 * w / h\n",
    "    else:\n",
    "        nw = 224\n",
    "        nh = 224 * h / w\n",
    "    \n",
    "    img = img.resize((int(nw), int(nh)))\n",
    "\n",
    "    try:\n",
    "        num_gt_instances = len(anno_dict[image][\"obj\"])\n",
    "        for i, (obj, coord) in enumerate(zip(anno_dict[image][\"obj\"], anno_dict[image][\"coord\"])):\n",
    "            cat = str(obj)\n",
    "            if ('shoot' in cat): #or ('crouch' in cat) or ('grab (a person)' in cat) or ('smoke' in cat):\n",
    "                xmin, ymin, xmax, ymax = coord[0], coord[1], coord[2], coord[3]\n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    img, ymin, xmin, ymax, xmax,\n",
    "                    color = 'Green',\n",
    "                    display_str_list=[cat],\n",
    "                    use_normalized_coordinates=False\n",
    "                )\n",
    "                print_pred = True\n",
    "                print(\"image id: \", image)\n",
    "            else:\n",
    "                print_pred = False\n",
    "        if print_pred:\n",
    "            for j, (obj, coord) in enumerate(zip(pred_dict[image][\"obj\"], pred_dict[image][\"coord\"])):\n",
    "                cat = str(obj)\n",
    "                xmin, ymin, xmax, ymax = coord[0], coord[1], coord[2], coord[3]\n",
    "                vis_utils.draw_bounding_box_on_image(\n",
    "                    img, ymin, xmin, ymax, xmax,\n",
    "                    color = 'Yellow',\n",
    "                    display_str_list=[cat],\n",
    "                    use_normalized_coordinates=False,\n",
    "                    margin2=20\n",
    "                )\n",
    "            display(img)\n",
    "            print(number)\n",
    "        # print(\"number of GT instances: \", num_gt_instances)\n",
    "        # print(\"number of predictions: \", num_predicted_instances)\n",
    "            number += 1\n",
    "        # del img\n",
    "    except:\n",
    "        print(\"excluded key frame encountered, skip to the next frame\")\n",
    "        # user_input = str(input())\n",
    "        # if user_input == \"q\": break\n",
    "        # else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(anno_dict[image]['obj'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dict[image]['coord'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anno_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
